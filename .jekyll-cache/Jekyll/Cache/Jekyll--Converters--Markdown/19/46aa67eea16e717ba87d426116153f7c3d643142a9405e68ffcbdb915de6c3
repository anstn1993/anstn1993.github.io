I"Q<p>Tsutomu Tone의 [성공과 실패를 결정하는 1%의 네트워크 원리]라는 책을 읽고 개인적으로 정리한 포스트 입니다. ‘5장 서버측의 LAN에는 무엇이 있는가’를 정리했습니다.</p>

<h1 id="5장-서버측의-lan에는-무엇이-있는가">5장. 서버측의 LAN에는 무엇이 있는가.</h1>

<hr />

<p>5장에서는 방화벽, 캐시, 콘텐츠 배포 서비스 등과 같은 서버 측의 네트워크 구조를 살펴봅니다.</p>

<h3 id="웹-서버의-설치-장소">웹 서버의 설치 장소</h3>

<p>웹 서버는 크게 회사 내의 LAN과 데이터센터에 설치됩니다. 각각에 대해 간단하게 살펴보겠습니다.</p>

<h4 id="사내에-설치하는-경우">사내에 설치하는 경우</h4>

<p>웹 서버가 사내 LAN에 설치되어 인터넷에서 직접 엑세스하는 경우입니다. 결론부터 말하면 현재 이 방법은 여러 문제들 때문에 주류가 아닙니다. 문제 중 하나는 IP 부족입니다. 사내 LAN에 서버를 설치하면 글로벌 주소를 할당해야 하지만 그럴 여유가 없습니다. 또 하나의 문제는 보안적으로 취약하다는 것입니다. 인터넷에서 바로 접근이 가능하기 때문에 서버 자체적으로 방어를 강화하는 것 말고는 어떠한 보안적 대비를 할 수 없습니다.</p>

<p>그래서 최근에는 웹 서버 앞에 <strong>방화벽</strong>을 두는 것이 일반적입니다. 방화벽은 특정 서버에서 동작하는 특정 애플리케이션에 접근하는 패킷만 통과시키고 그 외의 패킷은 모두 차단하는 관문 역할을 합니다. 이렇게 되면 최소한으로면 애플리케이션의 접근 범위를 설정해서 보안 구멍이 생길 위험성을 줄일 수 있습니다. 물론 접근을 허용한 애플리케이션에 보안 구멍이 있으면 공격받을 위험성은 여전히 존재합니다. 그렇지만 방화벽이 없는 것보단 낫습니다.</p>

<h4 id="데이터센터에-웹-서버를-설치하는-경우">데이터센터에 웹 서버를 설치하는 경우</h4>

<p>프로바이더 등의 데이터센터에 웹 서버를 설치하는 경우입니다. 데이터센터는 인터넷 중심부의 고속 회선으로 접속되어있기 때문에 여기에 서버를 설치하면 고속으로 액세스가 가능합니다. 트래픽이 많은 서비스를 운영할 때 효과적입니다. 또한 데이터센터는 그 역할 상 내진 설계, 자가 발전 장치 비치, 24시간 입/퇴실 등은 물론이고 기기의 가동 상태 감시, 방화벽 설치 운영, 부정 침입 감시 등의 부가 서비스도 제공하기 때문에 안전성도 높습니다.</p>

<h3 id="방화벽의-원리와-동작">방화벽의 원리와 동작</h3>

<p>방화벽의 기본 원리는 허용된 서버와 그 서버 내의 허용된 애플리케이션에 액세스하는 패킷만 통과시키고 나머지는 전부 차단하는 것입니다. 원리는 간단하지만 그것을 네트워크에는 다양한 종류의 패킷이 흐르기 때문에 구현하는 것은 그리 간단하지 않습니다. 그래서 그동안 패킷 필터링형, 애플리케이션 게이트웨이형, 서킷 게이트웨이형과 같은 다양한 방법이 고안되었지만 성능, 가격, 사용 편의성 등의 이유로 <strong>패킷 필터링형</strong>이 가장 보편화되었습니다. 그래서 이 장에서는 패킷 필터링형에 초점을 맞추겠습니다. 서버의 구성은 공개용 LAN과 사내 LAN이 분리되어있고, 웹 서버는 공개용 LAN에 접속되어있다고 가정하겠습니다.</p>

<h4 id="패킷-필터링의-조건-설정">패킷 필터링의 조건 설정</h4>

<p>인터넷에서 공개용 LAN의 웹 서버로의 액세스는 허용하고, 웹 서버에서 인터넷으로의 액세스는 금지(부정 소프트웨어로 인한 감염을 막기 위함)하기 위한 패킷 필터링의 조건 설정을 어떻게 해야할지 살펴보겠습니다.</p>

<p><strong>서버의 접근 액세스 허용 우무는 IP헤더의 송신처 IP와 수신처 IP로 판단</strong>합니다. 송신처 IP는 시점이 되고 수신처 IP는 종점이 됩니다. 인터넷에서 웹 서버를 향해 흐르는 패킷이면 패킷의 시점을 지정할 수는 없지만, 흐름의 종점은 웹 서버가 되기 때문에 수신처 IP가 종점과 일치하는 패킷은 통과시킨다는 조건을 설정하면 됩니다. 만약 송신처 IP 주소에 따라 시점을 지정할 수있으면 이것도 조건에 추가하면 됩니다. 그런데 TCP 통신은 항상 패킷을 잘 받았다는 수신 확인 응답 구조를 가지기 때문에 패킷이 웹서버로 들어왔으면 웹서버에서 인터넷으로 흐르는 패킷도 생기게 됩니다. 이때는 반대로 웹 서버의 IP 주소를 송신처 IP로 설정해두면 됩니다.</p>

<p>IP 주소로만 필터링을 하면 웹 서버의 애플리케이션 액세스를 필터링할 수는 없습니다. <strong>애플리케이션의 액세스 허용 유무는 TCP 헤더의 송신처 포트번호와 수신처 포트번호로 판단</strong>합니다. 수신처 IP 주소 및 송신처 IP 주소에 수신처 포트가 80번으로 설정되어있으면 수신처 주소가 웹 서버의 주소와 일치하면서 포트 번호가 80번인 패킷은 통과하게 됩니다. 또한 송신처 IP 주소가 웹 서버의 주소와 일치하고 송신처 포트 번호가 80번인 패킷도 통과하도록 설정하면 됩니다.</p>

<p>그런데 아직 이것으로도 좀 부족합니다. IP 주소와 포트 번호만으로는 접속 방향을 판단할 수는 없기 때문입니다. 우리의 목표는 <strong>웹 서버에서 인터넷측으로 패킷이 흐르는 것을 막는 것인데, 이 판단에 사용하는 데이터가 바로 TCP 헤더의 컨트롤 비트</strong>입니다. TCP는 최초에 접속 단계에서 3개의 패킷이 흐릅니다. 이때 가장 첫번째 패킷은 SYN비트는 1, ACK비트는 0입니다. 그런데 송신처 주소와 포트번호가 웹 서버의 것으로 설정된 상태에서 SYN비트가 1이고 ACK 비트는 0인 경우만 차단을 해주면 접속 동작이 필히 실패하게 될 것이고 이를 통해 웹 서버에서 인터넷으로 액세스하는 동작을 정지시킬 수 있습니다.
그런데 UDP의 경우 TCP와 달리 접속 단계가 없기 때문에 TCP처럼 컨트롤 비트에 의한 액세스 방향을 판별할 수 없습니다. 대표적으로 DNS 서버의 경우 사내에서 DNS 서버에 액세스하는 것은 허가하면서 인터넷에서 사내의 DNS 서버에 액세스하는 패킷은 차단한다는 조건 설정을 할 수 없습니다. 이런 경우에는 어느 정도 위험을 각오하고 모든 애플리케이션에 대한 패킷을 모두 통과시키거나 전면적으로 차단하는 방법을 택해야 합니다.(패킷 필터링 이외의 방법을 사용하면 UDP의 접속 방향을 판단할 수 있는 경우도 있습니다.)</p>

<p>지금까지 인터넷과 공개용 서버 LAN을 오가는 패킷에 대한 조건 설정을 살펴봤습니다. 그런데 이때 사내 LAN과 인터넷, 사내 LAN과 공개 서버용 LAN 사이를 오가는 패킷 조건도 설정해야 합니다. 이때 조건의 누락이 없는지 잘 살펴야 합니다. 사내 LAN과 공개 서버용 LAN 사이의 패킷은 자유롭게 오갈 수 있게 하려고 수신처 IP 주소가 공개 서버용 LAN과 일치하는 패킷을 전부 통과시키게 설정했다고 가정하겠습니다. 근데 이때 깜박하고 송신처 IP 주소를 조건으로 주지 않으면 이 조건 행으로 인해서 인터넷에서 흘러온 패킷도 무조건 공개 서버용 LAN으로 유입이 가능해지는 취약점이 발생하게 됩니다.</p>

<h4 id="방화벽의-주소-변환-기능">방화벽의 주소 변환 기능</h4>

<p>패킷 필터링 방화벽은 주소 변환의 기능도 가지고 있습니다. 사내 LAN과 인터넷을 왕래하는 패킷은 주소 변환 설정이 필요합니다. 이 주소 변환 설정을 이용하면 당연히 인터넷에서 사내 LAN으로는 액세스할 수 없게 됩니다. 그래서 사내 LAN에 대한 액세스 금지 패킷 필터링 조건은 설정할 필요가 없습니다.</p>

<h4 id="방화벽도-사실은-라우터다">방화벽도 사실은 라우터다</h4>

<p><strong>방화벽은 패킷 필터링이라는 부가적인 기능을 수행하는 라우터</strong>라고 생각해야 합니다. 단지 필터링의 조건이 복잡하고, 버린 패킷을 기록으로 남기는 등의 작업이 라우터에게 부담스러운 작업이라서 전용 하드웨어나 소프트웨어를 사용하는 것 뿐입니다. 만약 조건이 매우 간단하고, 버린 패킷에 대한 기록도 필요없다면 패킷 필터링 기능을 가진 라우터를 방화벽으로 사용해도 무방합니다.</p>

<h3 id="서버의-부하-분산">서버의 부하 분산</h3>

<p>서버에 액세스가 증가할 때는 서버로 통하는 회선을 빠르게 하면 효과적이지만, 아무리 회선의 성능이 좋아도 서버의 처리 능력이 그에 상응하지 못하면 부하는 점점 증가하게 됩니다. 이때 서버의 하드웨어를 고성능으로 교체하는 scale up 방식으로 부하를 줄일 수 있겠지만 서버 한대로 모든 요청을 견뎌내는 것은 한계가 있습니다. 그래서 결국은 서버 한대당 요청 처리량을 줄이는 scale out 방식으로 문제를 해결해야 합니다. 이를 분산 처리라고 합니다. 분산 처리를 하는 방법은 크게 DNS 서버를 통해 요청을 분산하는 방법과 로드 밸런서 같은 부하분산장치를 통해 요청을 분산하는 방법이 있습니다. 각각의 방법을 살펴보겠습니다.</p>

<h4 id="dns-서버를-통한-부하-분산">DNS 서버를 통한 부하 분산</h4>

<p>DNS 서버에 같은 도메인에 대해 여러 웹 서버 IP 주소를 등록해두면 DNS 서버는 해당 도메인 조회가 있을 때마다 라운드 로빈 방식으로 IP 주소를 돌려주게 됩니다. 가령 <code class="language-plaintext highlighter-rouge">www.cyber.co.kr</code>이라는 도메인에 대해 <code class="language-plaintext highlighter-rouge">192.0.2.60</code>, <code class="language-plaintext highlighter-rouge">192.0.2.70</code>, <code class="language-plaintext highlighter-rouge">192.0.2.80</code>이라는 3개의 주소를 등록하면 해당 도메인을 요청할 때 3개의 주소를 순환하면서 반환해줍니다. 이렇게 하면 균등하게 부하를 분산할 수 있습니다.</p>

<p>하지만 이 방식에는 결점들이 존재합니다. 먼저 서버 한 대에 문제가 생기더라도 그 서버에 요청이 가게 된다는 것입니다. 그 외에도 복수의 페이지에 걸쳐서 서버와 클라이언트 간의 상호작용이 일어나야 하는 경우에도 요청마다 요청이 가는 웹 서버가 달라지게 됩니다. 예를 들면 쇼핑 사이트에서 여러 페이지에 걸쳐서 주문 정보를 받는 경우가 있습니다.</p>

<h4 id="부하-분산-장치를-통한-부하-분산">부하 분산 장치를 통한 부하 분산</h4>

<p>DNS 서버를 통한 방식의 문제를 피하기 위해 로드 밸런서 같은 부하 분산 장치 같은 기기가 고안됐습니다. 부하 분산 장치는 실제 웹 서버 앞에서 프록시 형태로 위치하여 요청을 받아서 적절하게 분배해줍니다. 그래서 DNS 서버에 등록할 때는 이 부하 분산 장치의 IP 주소를 등록해줍니다. 그러면 클라이언트에서 보낸 요청은 부하 분산 장치가 받고 웹 서버로 전달해주게 됩니다.
이때 어떤 웹 서버에 요청을 전달할지에 대한 판단 근거는 여러 가지가 있습니다. 만약 복수 페이지에 걸친 상호작용이 필요한 경우가 아니면 웹 서버의 부하 상태가 판단 근거가 될 것입니다. 부하 상태를 판단하기 위해 주기적으로 CPU나 메모리 사용률 등을 수집하거나, 시험 패킷을 주기적으로 보내서 응답 시간으로 판단할 수도 있습니다.
만약 복수 페이지에 걸친 상호작용이 필요하다면 웹 서버의 부하와 무관하게 이전 리퀘스트와 같은 웹 서버에 요청을 전달해야 합니다. 그런데 HTTP는 기본적으로 요청에 대한 응답이 일어나면 연결을 끊도록 고안되었기 때문에 이전 리퀘스트와의 관계를 파악하기가 힘듭니다. 요청의 송신처 IP 주소가 같으면 일련의 요청이라고 판단할 수도 없습니다. 앞서 말했듯이 부하 분산 장치는 프록시 서버이고 프록시는 요청을 전달할 때 송신처 IP 주소를 자신의 IP로 바꿔버리기 때문입니다. 또한 주소 변환을 하는 경우에도 송신처 IP의 주소는 공인 IP 주소가 되기 때문에 정확한 클라이언트를 판별할 수 없습니다.
이런 상황에서 나온 방법은 form에 입력한 데이터를 보낼 때 전후의 관련성을 나타내는 정보를 추가해서 보내거나 쿠키를 이용하는 것입니다. 부하 분산 장치는 이런 정보가 존재하면 이전과 같은 웹 서버에, 그렇지 않으면 부하가 적은 웹 서버에 요청을 전달합니다.</p>

<h3 id="캐시-서버를-이용한-부하-분산">캐시 서버를 이용한 부하 분산</h3>

<p>캐시서버를 이용하여 부하 분산을 할 수도 있습니다. 캐시서버는 클라이언트와 서버 사이에 프록시 형태로 자리하여 웹 서버 액세스 동작을 중개합니다. 캐시서버는 웹 서버로부터 받은 응답 데이터를 디스크에 저장해두고 클라이언트가 같은 데이터를 요청하면 웹 서버 대신 저장해둔 데이터를 반환하는 역할을 수행합니다. 이렇게 하면 웹 서버는 매 요청마다 동적 페이지를 생성하지 않아도 되기 때문에 부하가 많이 줄어듭니다. 요청마다 매번 달라지는 데이터 같은 경우에는 캐시서버에 저장할 수 없지만 그래도 저장이 가능한 것은 최대한 캐시 서버에 저장하는 것이 좋습니다.</p>

<h4 id="캐시-서버는-갱신일로-데이터를-관리한다">캐시 서버는 갱신일로 데이터를 관리한다</h4>

<p>캐시 서버의 동작을 좀 더 자세히 살펴보겠습니다. 캐시 서버는 부하분산 장치와 마찬가지로 캐시 버서를 웹 서버 대신 DNS 서버에 등록하게 됩니다. 그래서 클라이언트에서 요청을 보내면 캐시 서버가 1차적으로 받게 됩니다. 캐시 서버는 요청 데이터가 자신의 캐시에 저장되어 있는지 조사합니다.
먼저 데이터가 저장되지 않은 경우부터 살펴보겠습니다. 캐시 서버는 웹서버로 데이터 요청을 보냅니다. 이때 요청이 캐시 서버를 경유해서 왔다는 것을 알리기 위해 헤더에 ‘Via’라는 필드에 캐시 서버의 도메인 주소를 채워서 웹 서버로 보내게 됩니다. 이때 웹 서버가 한 대라면 웹 서버의 도메인이나 IP 주소를 캐시 서버에 설정해두고 무조건 거기로 전송하면 되지만, 웹 서버가 여러대인 경우에는 다른 방법이 필요합니다. 대표적인 방법은 요청 헤더의 URI별로 웹 서버를 할당하는 것입니다. 예를 들어 요청 URI가 ‘/dir1/’로 시작하면 ‘www1.lab.cyber.co.kr’로, 요청 URI가 ‘/dir2/’로 시작하면 ‘www2.lay.cyber.co.kr’로 요청을 전달하는 방식입니다. 요청을 받은 웹 서버는 캐시 서버로 응답을 해주게 됩니다. 그러면 캐시 서버는 데이터를 저장한 후 클라이언트에게 응답을 해주게 됩니다. 이 응답 헤더에도 ‘Via’ 필드에 캐시 서버의 도메인을 추가하게 됩니다. 참고로 ‘Via’ 헤더는 필수가 아닙니다. 캐시 서버의 설정에 따라 이 헤더를 추가하지 않기도 합니다.
이번에는 캐시 서버에 요청 데이터가 저장되어 있는 경우를 살펴보겠습니다. 데이터가 존재하더라도 웹 서버측에 데이터가 변경되었는지 확인을 해야 합니다. 이 확인용 요청을 보낼 때 ‘If-Modified-Since’ 헤더를 추가하여 웹 서버에 전송하게 됩니다. 웹 서버는 저 헤더 필드의 값과 데이터의 최종 갱신 일시를 비교하여 변경이 없으면 ‘304 Not Modified’ 응답을 돌려주게 됩니다. 캐시 서버에 데이터가 있어도 웹 서버가 이렇게 데이터 변경 확인 요청을 받긴 하지만 실제 데이터 요청보다는 훨씬 부담이 적습니다. 이제 캐시 서버는 자신이 저장한 데이터가 최신 데이터라는 것을 확인했으니 그 데이터를 클라이언트에게 응답해주게 됩니다. 이때 응답은 데이터가 없었을 때의 응답 메시지와 동일합니다. 만약 웹 서버측의 데이터가 변경되었다면 그때부터는 캐시서버에 데이터가 저장되지 않았을 때의 플로우와 완전히 동일합니다. 웹 서버가 데이터를 반환하게 되기 때문에 캐시서버는 응답 헤더에 ‘Via’ 필드를 추가해서 클라이언트에게 응답합니다.</p>

<h4 id="프록시의-유형">프록시의 유형</h4>

<p>캐시 서버가 프록시 방식으로 동작하는 것을 확인했습니다. 사실 이 프록시는 프록시 서버가 어디에 위치하는지에 따라 포워드 프록시, 리버스 프록시, 트랜스패어런트 프록시로 분류할 수 있습니다. 하나씩 살펴보겠습니다.</p>

<p>포워드 프록시는 클라이언트측에 캐시 서버를 두는 방식입니다. 포워드 프록시는 초기의 캐시 서버 형태였습니다. 서버측에 설치하는 캐시서버와 같지만 이 당시의 포워드 프록시는 방화벽을 수행하는 목적으로도 사용되었습니다. 앞서 클라이언트쪽에 프록시 형태로 방화벽을 둠으로써 인터넷에서 사내 망으로의 부정 침입은 막으면서도 사내 망에서 인터넷으로의 액세스는 가능했던 것을 살펴봤습니다. 포워드 프록시는 요청의 데이터를 조사하기 때문에 위험한 사이트에 대한 액세스는 금지할 수 있습니다. 여기에 캐시 기능까지 추가하여 속도까지 잡았습니다.
포워드 프록시는 클라이언트에 존재하기 때문에 보통 브라우저에서 프록시 서버 항목을 설정합니다. 프록시 서버 url을 설정하게 되면 브라우저의 url 입력 상자에 어떤 url을 입력하든 무조건 요청은 포워드 프록시 서버로 하게 됩니다. 그리고 url의 입력 상자에 입력했던 url은 요청 헤더의 ‘URI’로 기록하여 포워드 프록시에게 요청 대상 웹서버를 통지합니다. 이처럼 클라이언트 프록시는 요청 대상 웹서버의 url을 URI로 설정하기 때문에 웹 서버가 여러 대인 경우 서버 쪽에 캐시 서버를 두는 경우와 다르게 대상 웹서버를 찾을 필요가 없습니다. 대상 서버를 찾는 일은 부하 분산 장치가 해야 할 일이기 때문이죠.</p>

<p>리버스 프록시는 앞서 살펴본 서버측에 캐시 서버를 두는 형태입니다. 리버스 프록시는 포워드 프록시와 달리 브라우저에 프록시 서버 설정을 하지 않아도 되기 때문에 잘못된 브라우저 설정으로 인한 장애를 걱정할 필요가 없습니다. 이미 살펴봤지만 포워드 프록시와 다르게 리퀘스트 메시지의 URI 부분에 쓰여있는 리소스와 웹 서버를 대응시켜서 요청을 받은 캐시 서버가 어떤 웹 서버로 요청을 전달해야 할지 결정하게 됩니다.</p>

<p>트랜스페어런트 프록시는 IP 헤더에 있는 수신처 IP로 액세스 대상 웹 서버를 판단하는 방식입니다. 이런 방식이면 일반적인 리퀘스트 메시지를 전송할 수 있기 때문에 포워드 프록시처럼 브라우저에 프록시 서버 설정을 하지 않아도 되고 리버스 프록시처럼 URI와 웹 서버를 대응시킬 필요도 없습니다. 이를 가능하게 하기 위해서 클라이언트에서 웹 서버로 요청 메시지가 흘러가는 길에 트랜스페어런트 프록시를 설치합니다. 메시지가 프록시를 통과할 때 메시지를 가로채어서 대상 웹서버로 보내는 것입니다. 만약 요청 메시지가 흐르는 길이 많으면 길이 한 개로 수렴하는 형태로 네트워크를 구성하고 수렴되는 곳에 트랜스페어런트 프록시를 설치하는 것이 일반적입니다. 보통 인터넷에 연결하는 액세스 회선 부분이 이런 형태로 구성되어 있어서 액세스 회선 부분에 설치하기도 합니다.</p>
:ET